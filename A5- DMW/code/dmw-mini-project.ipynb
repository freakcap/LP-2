{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nimport xgboost\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_log_error,r2_score,accuracy_score,confusion_matrix\n\nimport datetime\nfrom sklearn import preprocessing\nsns.set()\n\ntrain = pd.read_csv('/kaggle/input/dmwmini/train.csv')\ntest = pd.read_csv('/kaggle/input/dmwmini/test.csv')\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ntrain['date'] = pd.to_datetime(train['date'],format='%m/%d/%Y')\n# train['month'] = train['date'].dt.month\n# train['dayofweek'] = train['date'].dt.dayofweek\n# train['year'] = train['date'].dt.year\n# train['day'] = train['date'].dt.day\n# train['dayofyear'] = train['date'].dt.dayofyear\n# train['weekofyear'] = train['date'].dt.weekofyear\n\ntrain.drop(['date'], axis=1, inplace=True) \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train, col='game_result')\ng.map(plt.hist, 'Elo', bins=20)\n\ng = sns.FacetGrid(train, col='game_result')\ng.map(plt.hist, 'opp_Elo', bins=20)\n\ng = sns.FacetGrid(train, col='game_result')\ng.map(plt.hist, 'total_crowd', bins=20)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for index,row in train.iterrows():\n    if(row['bet_ratio']<0.13 or row['bet_ratio']>0.97):\n        train.drop(index,inplace=True)  \n    elif(row['opp_Elo']<1200 and row['opp_Elo']>1750):\n        train.drop(index,inplace=True)  \n    elif(row['Elo']<1175 and row['Elo']>1780):\n        train.drop(index,inplace=True)\n    elif(row['win_equivalent']<14 or row['win_equivalent']>68):\n        train.drop(index,inplace=True)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.concat([train,pd.get_dummies(train['team_id'], prefix='team_id',dummy_na=False)],axis=1)\n# train = pd.concat([train,pd.get_dummies(train['opp_team_id'], prefix='opp_team_id',dummy_na=False)],axis=1)\n\ntrain['team_id'] = train['team_id'].str.extract('(\\\\d+)', expand=True)\ntrain['opp_team_id'] = train['opp_team_id'].str.extract('(\\\\d+)', expand=True)\n\ntrain['team_id']=train['team_id'].astype(int)\ntrain['opp_team_id']=train['opp_team_id'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['bet_ratio'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['Elo'].fillna(train.groupby('team_id')['Elo'].transform('median'), inplace=True)\n# train['opp_Elo'].fillna(train.groupby('opp_team_id')['opp_Elo'].transform('median'), inplace=True)\n# train['win_equivalent'].fillna(train.groupby('season_end')['win_equivalent'].transform('median'), inplace=True)\n\ntrain[\"temp\"] = train[\"team_id\"].map(str) + train[\"season_end\"].map(str)\ntrain[\"temp1\"] = train[\"opp_team_id\"].map(str) + train[\"season_end\"].map(str)\n\ntrain['Elo'].fillna(train.groupby('temp')['Elo'].transform('median'), inplace=True)\ntrain['opp_Elo'].fillna(train.groupby('temp1')['opp_Elo'].transform('median'), inplace=True)\ntrain['win_equivalent'].fillna(train.groupby('season_end')['win_equivalent'].transform('median'), inplace=True)\n\ntrain = train.dropna(axis = 0, how ='any')\n\ntrain.drop(['temp'], axis=1, inplace=True) \ntrain.drop(['temp1'], axis=1, inplace=True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['diff']=train.Elo-train.opp_Elo\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntrain.drop(['game_seq'], axis=1, inplace=True)  \n# train.drop(['season_end'], axis=1, inplace=True)        \n# train.drop(['season_game_seq'], axis=1, inplace=True) \n# train.drop(['playoff'], axis=1, inplace=True)\ntrain.drop(['team_id'], axis=1, inplace=True) \n# train.drop(['Elo'], axis=1, inplace=True)\ntrain.drop(['opp_team_id'], axis=1, inplace=True)\n# train.drop(['opp_Elo'], axis=1, inplace=True) \n# train.drop(['win_equivalent'], axis=1, inplace=True) \n# train.drop(['bet_ratio'], axis=1, inplace=True) \n# train.drop(['home_crowd'], axis=1, inplace=True) \ntrain.drop(['opp_crowd'], axis=1, inplace=True) \n# train.drop(['total_crowd'], axis=1, inplace=True) \n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labelencoder = LabelEncoder()\ntrain['playoff'] = labelencoder.fit_transform(train['playoff'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[\"game_result\"]\nX = train.drop([\"game_result\"],axis=1)\n\n# scaler=MinMaxScaler()\n# X = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\nx_train , x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaler=StandardScaler()\n# X_train = scaler.fit_transform(x_train)\n# X_test = scaler.transform(x_test)\nX_train = x_train\nX_test = x_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True, n_estimators= 100)\nrfc.fit(X_train,y_train)\n\ny_pred= rfc.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nacc= accuracy_score(y_test,y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nlmse = mean_squared_log_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nlrmse = np.sqrt(lmse)\nprint(\"Random Forest Classifier :\")\nprint(\"R2 score:\", r2)\nprint(\"ACC:\", acc)\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"LMSE:\", lmse)\nprint(\"RMSE:\", rmse)\nprint(\"LRMSE:\", lrmse)\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Win','Lose'],\n                      title='Confusion matrix for random forest classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknc = KNeighborsClassifier(n_neighbors=5)\nknc.fit(X_train, y_train)\n\ny_pred= knc.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nacc= accuracy_score(y_test,y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nlmse = mean_squared_log_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nlrmse = np.sqrt(lmse)\nprint(\"KN neighbors Classifier :\")\nprint(\"R2 score:\", r2)\nprint(\"ACC:\", acc)\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"LMSE:\", lmse)\nprint(\"RMSE:\", rmse)\nprint(\"LRMSE:\", lrmse)\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Win','Lose'],\n                      title='Confusion matrix for K Neighbours classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\ny_pred= lr.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nacc= accuracy_score(y_test,y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nlmse = mean_squared_log_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nlrmse = np.sqrt(lmse)\nprint(\"Logistic Regression :\")\nprint(\"R2 score:\", r2)\nprint(\"ACC:\", acc)\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"LMSE:\", lmse)\nprint(\"RMSE:\", rmse)\nprint(\"LRMSE:\", lrmse)\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Win','Lose'],\n                      title='Confusion matrix for logistic regression')"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\ny_pred= lr.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nacc= accuracy_score(y_test,y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nlmse = mean_squared_log_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nlrmse = np.sqrt(lmse)\nprint(\"Logistic Regression :\")\nprint(\"R2 score:\", r2)\nprint(\"ACC:\", acc)\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"LMSE:\", lmse)\nprint(\"RMSE:\", rmse)\nprint(\"LRMSE:\", lrmse)\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Win','Lose'],\n                      title='Confusion matrix for logistic regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn import svm\n# svc = svm.SVC(kernel='linear') \n# svc.fit(X_train, y_train)\n\n# y_pred= svc.predict(X_test)\n# r2 = r2_score(y_test, y_pred)\n# acc= accuracy_score(y_test,y_pred)\n# mae = mean_absolute_error(y_test, y_pred)\n# mse = mean_squared_error(y_test, y_pred)\n# lmse = mean_squared_log_error(y_test, y_pred)\n# rmse = np.sqrt(mse)\n# lrmse = np.sqrt(lmse)\n# print(\"Support Vector Classifier :\")\n# print(\"R2 score:\", r2)\n# print(\"ACC:\", acc)\n# print(\"MAE:\", mae)\n# print(\"MSE:\", mse)\n# print(\"LMSE:\", lmse)\n# print(\"RMSE:\", rmse)\n# print(\"LRMSE:\", lrmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgc = xgboost.XGBClassifier(random_state = 1, max_depth = 15,n_estimators = 200, min_samples_split = 2, min_samples_leaf = 1)\nxgc=xgc.fit(X_train, y_train)\n\ny_pred= xgc.predict(X_test)\nr2 = r2_score(y_test, y_pred)\nacc= accuracy_score(y_test,y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nlmse = mean_squared_log_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nlrmse = np.sqrt(lmse)\nprint(\"XFB Classifier :\")\nprint(\"R2 score:\", r2)\nprint(\"ACC:\", acc)\nprint(\"MAE:\", mae)\nprint(\"MSE:\", mse)\nprint(\"LMSE:\", lmse)\nprint(\"RMSE:\", rmse)\nprint(\"LRMSE:\", lrmse)\ncnf_matrix = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Win','Lose'],\n                      title='Confusion matrix for XGBoost classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_importances = pd.DataFrame(lr.feature_importances_,\n#                                    index = X_train.columns,\n#                                     columns=['importance']).sort_values('importance',ascending=False)\n# feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_gs = False # make true for grid search\n\nif run_gs:\n    parameter_grid = {\n                 'max_depth' : [4, 6, 8],\n                 'n_estimators': [50, 10, 20],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 'bootstrap': [True, False],\n                 }\n    forest = RandomForestClassifier()\n    cross_validation = StratifiedKFold(n_splits=1)\n\n    grid_search = GridSearchCV(forest,\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=cross_validation,\n                               verbose=1,\n                               n_jobs=-1\n                              )\n\n    grid_search.fit(X_train, y_train)\n    model = grid_search\n    parameters = grid_search.best_params_\n\n    print('Best score: {}'.format(grid_search.best_score_))\n    print('Best parameters: {}'.format(grid_search.best_params_))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\ntest['date'] = pd.to_datetime(test['date'],format='%m/%d/%Y')\n# test['month'] = test['date'].dt.month\n# test['dayofweek'] = test['date'].dt.dayofweek\n# test['year'] = test['date'].dt.year\n# test['day'] = test['date'].dt.day\n# test['dayofyear'] = test['date'].dt.dayofyear\n# test['weekofyear'] = test['date'].dt.weekofyear\n\ntest.drop(['date'], axis=1, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test = pd.concat([test,pd.get_dummies(test['team_id'], prefix='team_id',dummy_na=False)],axis=1)\n# test = pd.concat([test,pd.get_dummies(test['opp_team_id'], prefix='opp_team_id',dummy_na=False)],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['team_id'] = test['team_id'].str.extract('(\\\\d+)', expand=True)\ntest['opp_team_id'] = test['opp_team_id'].str.extract('(\\\\d+)', expand=True)\n\ntest['team_id']=test['team_id'].astype(int)\ntest['opp_team_id']=test['opp_team_id'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test['Elo'].fillna(test.groupby('team_id')['Elo'].transform('mean'), inplace=True)\n# test['opp_Elo'].fillna(test.groupby('opp_team_id')['opp_Elo'].transform('mean'), inplace=True)\n# test['win_equivalent'].fillna(test.groupby('season_end')['win_equivalent'].transform('mean'), inplace=True)\n\ntest[\"temp\"] = test[\"team_id\"].map(str) + test[\"season_end\"].map(str)\ntest[\"temp1\"] = test[\"opp_team_id\"].map(str) + test[\"season_end\"].map(str)\n\ntest['Elo'].fillna(test.groupby('temp')['Elo'].transform('median'), inplace=True)\ntest['opp_Elo'].fillna(test.groupby('temp1')['opp_Elo'].transform('median'), inplace=True)\ntest['win_equivalent'].fillna(test.groupby('season_end')['win_equivalent'].transform('median'), inplace=True)\n\ntest['Elo'].fillna(test.groupby('team_id')['Elo'].transform('mean'), inplace=True)\ntest['opp_Elo'].fillna(test.groupby('opp_team_id')['opp_Elo'].transform('mean'), inplace=True)\n\ntest = test.dropna(axis = 0, how ='any')\n\ntest.drop(['temp'], axis=1, inplace=True) \ntest.drop(['temp1'], axis=1, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for index,row in test.iterrows():\n    \n#     if(test.loc[index,'bet_ratio']<0.5):\n#         test.at[index,'Bet']=-1 \n#     if(test.loc[index,'bet_ratio']>0.8):\n#         test.at[index,'Bet']=1\n#     if(test.loc[index,'bet_ratio']>=0.5 and test.loc[index,'bet_ratio']<=0.8):\n#         test.at[index,'Bet']=0\n#     if(test.loc[index,'Elo']<1350):\n#         test.at[index,'elo1']=-1 \n#     if(test.loc[index,'Elo']>1645):\n#         test.at[index,'elo1']=1\n#     if(test.loc[index,'Elo']>=1350 and test.loc[index,'Elo']<=1645):\n#         test.at[index,'elo1']=0\n#     if(test.loc[index,'opp_Elo']<1355):\n#         test.at[index,'elo2']=-1 \n#     if(test.loc[index,'opp_Elo']>1660):\n#         test.at[index,'elo2']=1\n#     if(test.loc[index,'opp_Elo']>=1355 and test.loc[index,'opp_Elo']<=1660):\n#         test.at[index,'elo2']=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['diff']=test.Elo-test.opp_Elo\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Id = test['Id']\n\ntest.drop(['Id'], axis=1, inplace=True)\ntest.drop(['game_seq'], axis=1, inplace=True)  \n# test.drop(['season_end'], axis=1, inplace=True)        \n# test.drop(['season_game_seq'], axis=1, inplace=True) \n# test.drop(['playoff'], axis=1, inplace=True)\ntest.drop(['team_id'], axis=1, inplace=True) \n# test.drop(['Elo'], axis=1, inplace=True)\ntest.drop(['opp_team_id'], axis=1, inplace=True)\n# test.drop(['opp_Elo'], axis=1, inplace=True) \n# test.drop(['win_equivalent'], axis=1, inplace=True) \n# test.drop(['bet_ratio'], axis=1, inplace=True) \n# test.drop(['home_crowd'], axis=1, inplace=True) \ntest.drop(['opp_crowd'], axis=1, inplace=True) \n# test.drop(['total_crowd'], axis=1, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labelencoder = LabelEncoder()\ntest['playoff'] = labelencoder.fit_transform(test['playoff'])\n# test['team_id'] = labelencoder.fit_transform(test['team_id'])\n# test['opp_team_id'] = labelencoder.fit_transform(test['opp_team_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaler=StandardScaler()\n# test = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_test=xgc.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_test=prediction_test.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_solution=pd.DataFrame(prediction_test,Id,columns=[\"game_result\"])\nprint(my_solution.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_solution.to_csv(\"sub123.csv\",index_label=[\"Id\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_solution['game_result'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}